<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css2?family=Lato&display=swap"
      rel="stylesheet">
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>Minimax Optimal Online Imitation Learning via Replay Estimation</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:title" content="Minimax Optimal Online Imitation Learning via Replay Estimation" />
	<meta property="og:description" content="When an online imitation learner attempts to match an expert's state distibution, finite-sample variance can lead them astray. We introduce replay estimation: a technique for smoothing estimates of expert state-action distributions before optimization. In short, one trains a policy via behavioral cloning, performs rollouts in the environment, and adds confidence-weighted trajectories to the demonstration set. We prove that in theory, this is the minimax optimal algorithm for IL in the finite sample regime and that this sort of data augmentation is able to noticeably improve practical performance when demonstrations are scarce." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="When an online imitation learner attempts to match an expert's state distibution, finite-sample variance can lead them astray. We introduce replay estimation: a technique for smoothing estimates of expert state-action distributions before optimization. In short, one trains a policy via behavioral cloning, performs rollouts in the environment, and adds confidence-weighted trajectories to the demonstration set. We prove that in theory, this is the minimax optimal algorithm for IL in the finite sample regime and that this sort of data augmentation is able to noticeably improve practical performance when demonstrations are scarce." />
    <meta property="twitter:title"         content="Minimax Optimal Online Imitation Learning via Replay Estimation" />
    <meta property="twitter:description"   content="When an online imitation learner attempts to match an expert's state distibution, finite-sample variance can lead them astray. We introduce replay estimation: a technique for smoothing estimates of expert state-action distributions before optimization. In short, one trains a policy via behavioral cloning, performs rollouts in the environment, and adds confidence-weighted trajectories to the demonstration set. We prove that in theory, this is the minimax optimal algorithm for IL in the finite sample regime and that this sort of data augmentation is able to noticeably improve practical performance when demonstrations are scarce." />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

</head>

<body>
<div class="container">
    <div class="title">
        Minimax Optimal Online Imitation Learning via Replay Estimation
    </div>

    <div class="venue">
        NeurIPS'22
    </div>

    <br><br>

    <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>1*</sup>
    </div>
    <div class="author">
        <a href="https://people.eecs.berkeley.edu/~nived.rajaraman/">Nived Rajaraman</a><sup>2*</sup>
    </div>
    <div class="author">
        Matthew Peng<sup>2</sup>
    </div>
    <br>
    <div class="author">
        <a href="http://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a><sup>3</sup>
    </div>
    <div class="author">
        <a href="https://www.ri.cmu.edu/ri-people/j-andrew-drew-bagnell/">Drew Bagnell</a><sup>4, 1</sup>
    </div>
    <div class="author">
        <a href="https://zstevenwu.com/">Steven Wu</a><sup>1</sup>
    </div>
    <br>
    <div class="author">
        <a href="https://people.eecs.berkeley.edu/~jiantao/">Jiantao Jiao</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="http://people.eecs.berkeley.edu/~kannanr/">Kannan Ramchandran</a><sup>2</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>CMU</div>
    <div class="affiliation"><sup>2&nbsp;</sup>UC Berkeley</div>
    <div class="affiliation"><sup>3&nbsp;</sup>Cornell University</div>
    <div class="affiliation"><sup>4&nbsp;</sup>Aurora Innovation</div>
    

    <br><br>

    <div class="links"><a href="https://arxiv.org/pdf/2205.15397.pdf"><i class="fa fa-file-text", style="font-size: 50px; padding-bottom: 10px"></i><br>[Paper]</a></div>
    <div class="links"><a href="https://youtu.be/3Ptog2HM35A"><i class="fa fa-play-circle" style="font-size: 50px; padding-bottom: 10px"></i><br>[Video]</a></div>
    <div class="links"><a href="https://github.com/gkswamy98/replay_est"><i class="fa fa-github" style="font-size: 50px; padding-bottom: 10px"></i><br>[Code]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/re_ffig.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        <br> When an online imitation learner attempts to match an expert's state distibution, finite-sample variance can lead them astray. We introduce replay estimation: a technique for smoothing estimates of expert state-action distributions before optimization. In short, one trains a policy via behavioral cloning, performs rollouts in the environment, and adds confidence-weighted trajectories to the demonstration set. We prove that in theory, this is the minimax optimal algorithm for IL in the finite sample regime and that this sort of data augmentation is able to noticeably improve practical performance when demonstrations are scarce.
    </p>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Online imitation learning is the problem of how best to mimic expert demonstrations, given access to the environment or an accurate simulator. Prior work has shown that in the <i>infinite</i> sample regime, exact moment matching achieves value equivalence to the expert policy. However, in the <i>finite</i> sample regime, even if one has no optimization error, empirical variance can lead to a performance gap that scales with $H^2 / N_{exp}$ for behavioral cloning and $H / \sqrt{N_{exp}}$ for online moment matching, where $H$ is the horizon and $N_{exp}$ is the size of the expert dataset. We introduce the technique of <i>replay estimation</i> to reduce this empirical variance: by repeatedly executing cached expert actions in a stochastic simulator, we compute a <i>smoother</i> expert visitation distribution estimate to match. In the presence of parametric function approximation, we prove a meta theorem reducing the performance gap of our approach to the <i>parameter estimation error</i> for offline classification (i.e. learning the expert policy). In the tabular setting or with linear function approximation, our meta theorem shows that the performance gap incurred by our approach achieves the optimal $\widetilde{O} \left( \min({H^{3/2}} / {N_{exp}}, {H} / {\sqrt{N_{exp}}} \right)$ dependency, under significantly weaker assumptions compared to prior work. We implement multiple instantiations of our approach on several continuous control tasks and find that we are able to significantly improve policy performance across a variety of dataset sizes.
    <br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/3Ptog2HM35A" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>
    <br>

    <hr>

    <h1>Key Insights</h1>

    <h2>1. The Challenge With Finite Samples in Online Imitation Learning</h2>
    <p style="width: 80%;">In imitation learning, we usually tell ourselves that the demonstrations we recieve are sufficient to charecterize expert behavior. However, with few enough samples, random variations can make certain states or actions seem more likely than they would in the infinite sample limit.</p>
    <img style="width: 60%;" src="./resources/finite.png" alt="Finite samples."/>
    <p style="width: 80%;">For example, in the above scenario, if we happen to sample a trajectory of the expert flying through the forest, our learner might think they also have to fly through this more dangerous area with the same probability. If we could query the expert for more demonstrations, this wouldn't be a problem. However, given this can be quite expensive, the question arises of how to make the most out of our data and simulator access.</p>

    <h2>2. Replay Estimation in Theory</h2>
    <p style="width: 80%;">There's two sources of uncertainty when we have a finite set of demonstrations. The first is the stochasticity in expert actions at a state. The second is the stochasticity in next states that one can end up in after an action. While we can't do much about the former without querying the expert, we can use the simulator to try and tackle the latter. We can <i>replay</i> cached expert actions in a simulator, adding in the transitions to our dataset. As long as we end up in a state where we also have seen an expert action, we can do this repeatedly to generate extra rollouts for free and smooth out our estimate of expert behavior.</p>
    <img style="width: 60%;" src="./resources/replay.png" alt="Finite samples."/>
    <p style="width: 80%;">The natural question at this point might be how we do this on continuous problems where we are unlikely to end up in exactly the same state more than once. Our insight is that continuous analog to states where we know the expert actions is states where behavioral cloning (which is only focued on matching expert actions) gets the expert action correct. We can therefore use simulated BC rollouts to augment our demonstration set before performing online IL. Putting it all together, <i>replay estimation</i> is the following algorithm:</p>
    <ol style="width: 80%; margin: auto; text-align: left;">
        <li>Train a policy via BC.</li>
        <li>Do rollouts in the environment of this policy.</li>
        <li>Compute confidence scores and add samples with a weight to the demonstration set.</li>
        <li>Run standard online IL, using the augmented dataset as input.</li>
    </ol> 
    <br>
    <p style="width: 80%;">We prove this is in fact the minimax optimal algorithm for online IL and enjoys a better-than-both worlds guarantee than both behavioral cloning and standard online IL algorithms like GAIL or MaxEnt IRL.<p/>

    <h2>3. Replay Estimation in Practice</h2>
    <p style="width: 80%;">We experiment with various methods for computing confidence scores and find that training an ensemble of BC learners and taking the max difference between any two members of this ensemble seems to work the best. We find that replay estimation (in orange) is able to out-perform both BC and a strong online IL baseline on harder versions of the PyBullet Gym tasks.</p>
    <img style="width: 30%;" src="./resources/r1.png" alt="Finite samples."/> <img style="width: 31%;" src="./resources/r2.png" alt="Finite samples."/>
    <!-- <h2>3. Understanding and Resolving the Latching Effect</h2>
    <p style="width: 80%;">In off-policy training, we are trying to learn a policy such that $\pi(a_t|s_1, a_1, \dots s_t) \approx p(a_t^E|s_1^E, a_1^E, \dots, s_t^E)$, where $E$ denotes expert. Now, for a lot of tasks, an expert might take similar actions over adjacent timesteps. So, instead of learning a complicated mapping from states to action, it is sufficient to learn a policy that just copies the last action to drive down training error. However, when this naive policy is combined with the learner's initially suboptimal past actions, we have a recipe for disaster: the learner just keeps repeating its own mistakes ad infinitum.</p>
    <p style="width: 80%;"> At test-time, our learner is attempting to sample from $p(a_t^E|s_1, a_1, \dots, s_t)$. However, we have no reason to believe the model we learned will generalize to our own history distribution. What's really happening here is covariate shift in the space of histories. When this is combined with unavoidable early errors, the learner can quickly go off the rails and stay there. We make this argument more formally in our paper.</p> -->
    <!-- <p style="width: 80%;"> We make this argument more formally in the paper.</p>
    <ul style="width: 80%; margin: auto; text-align: left;">
        <li>We show that the posterior over contexts generate under the off-policy graphical model weights the learner's own past choices as though they came from the expert (i.e. as though they had information about the unobserved context when they do not).</li>
        <li>We prove that off-policy learners have a value difference with respect to the expert governed by the sum of their errors over the horizon, while on-policy learners have one determined by their asymptotic error. Given <i>all</i> learners make errors early on as we're in an asymptotically realizable problem, this means that off-policy learners do not learn to recover from their own early mistakes even though it is possible for them to do so.</li>
      </ul> 
        <br>         -->
        <!-- <p style="width: 80%;"> We perform experiments in two domains. On a toy bandit problem, we see off-policy learners exihibit sharp <i>phase transitions</i> as far as consistency, in contrast to the uniform value equivalence of on-policy methods. On modifications of the PyBullet suite in which a key piece of information (e.g. the target speed) is hidden from the learner but not the expert, we see that adding history to an off-policy learner like behavioral cloning actually leads to <i>worse</i> performance, while see no such effect for on-policy algorithms like DAgger. We release all of our code at the link below.</p> -->
    <p style="width: 80%;">  We release all of our code at the link below. Our hope is that replay estimation will become a standard step before performing online IL.</p>
    <a href="https://github.com/gkswamy98/replay_est"><i class="fa fa-github" style="font-size: 50px; padding-bottom: 10px"></i><br>[Code]</a>
      <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/pdf/2205.15397.pdf">
            <img class="layered-paper-big" width="100%" src="./resources/paper.svg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Minimax Optimal Online Imitation Learning via Replay Estimation</h3>
        <p>Gokul Swamy*, Nived Rajaraman*, Sanjiban Choudhury, J. Andrew Bagnell, Zhiwei Steven Wu, Jiantao Jiao, Kannan Ramchandran</p>
        <pre><code>@misc{swamy2022minimax,
    title = {Minimax Optimal Online Imitation Learning via Replay Estimation},
    author = Gokul Swamy and Nived Rajaraman and Sanjiban Choudhury and J. Andrew Bagnell and Zhiwei Steven Wu and Jiantao Jiao and Kannan Ramchandran,
    year = {2022},
    booktitle = {Advances in Neural Information Processing Systems},
    volume = {34}
}</code></pre>
    </div>

    <br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
        and <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project, and
        adapted to be mobile responsive by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code we built on can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br>
</div>

</body>

</html>
